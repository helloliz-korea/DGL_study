{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장: 그래프 데이터 파이프라인\n",
    "## 4.1 DGLDataset 클래스\n",
    "<img src = 'https://data.dgl.ai/asset/image/userguide_data_flow.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dgl.data import DGLDataset\n",
    "\n",
    "class MyDataset(DGLDataset):\n",
    "    \"\"\" Template for customizing graph datasets in DGL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL to download the raw dataset\n",
    "    raw_dir : str\n",
    "        Specifying the directory that will store the\n",
    "        downloaded data or the directory that\n",
    "        already stores the input data.\n",
    "        Default: ~/.dgl/\n",
    "    save_dir : str\n",
    "        Directory to save the processed dataset.\n",
    "        Default: the value of `raw_dir`\n",
    "    force_reload : bool\n",
    "        Whether to reload the dataset. Default: False\n",
    "    verbose : bool\n",
    "        Whether to print out progress information\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 url=None,\n",
    "                 raw_dir=None,\n",
    "                 save_dir=None,\n",
    "                 force_reload=False,\n",
    "                 verbose=False):\n",
    "        super(MyDataset, self).__init__(name='dataset_name',\n",
    "                                        url=url,\n",
    "                                        raw_dir=raw_dir,\n",
    "                                        save_dir=save_dir,\n",
    "                                        force_reload=force_reload,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "    def download(self):\n",
    "        # download raw data to local disk\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # process raw data to graphs, labels, splitting masks\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get one example by index\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of data examples\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        # save processed data to directory `self.save_path`\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        # load processed data from directory `self.save_path`\n",
    "        pass\n",
    "\n",
    "    def has_cache(self):\n",
    "        # check whether there are processed data in `self.save_path`\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Raw 데이터 다운로드하기 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dgl.data.utils import download\n",
    "\n",
    "def download(self):\n",
    "    # path to store the file\n",
    "    file_path = os.path.join(self.raw_dir, self.name + '.mat')\n",
    "    # download file\n",
    "    download(self.url, path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data.utils import download, check_sha1\n",
    "\n",
    "def download(self):\n",
    "    # path to store the file\n",
    "    # make sure to use the same suffix as the original file name's\n",
    "    gz_file_path = os.path.join(self.raw_dir, self.name + '.csv.gz')\n",
    "    # download file\n",
    "    download(self.url, path=gz_file_path)\n",
    "    # check SHA-1\n",
    "    if not check_sha1(gz_file_path, self._sha1_str):\n",
    "        raise UserWarning('File {} is downloaded but the content hash does not match.'\n",
    "                          'The repo may be outdated or download may be incomplete. '\n",
    "                          'Otherwise you can create an issue for it.'.format(self.name + '.csv.gz'))\n",
    "    # extract file to directory `self.name` under `self.raw_dir`\n",
    "    self._extract_gz(gz_file_path, self.raw_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 데이터 프로세싱\n",
    "#### (1) 그래프 분류 데이터셋 프로세싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLDataset\n",
    "\n",
    "class QM7bDataset(DGLDataset):\n",
    "    _url = 'http://deepchem.io.s3-website-us-west-1.amazonaws.com/' \\\n",
    "           'datasets/qm7b.mat'\n",
    "    _sha1_str = '4102c744bb9d6fd7b40ac67a300e49cd87e28392'\n",
    "\n",
    "    def __init__(self, raw_dir=None, force_reload=False, verbose=False):\n",
    "        super(QM7bDataset, self).__init__(name='qm7b',\n",
    "                                          url=self._url,\n",
    "                                          raw_dir=raw_dir,\n",
    "                                          force_reload=force_reload,\n",
    "                                          verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        mat_path = self.raw_path + '.mat'\n",
    "        # process data to a list of graphs and a list of labels\n",
    "        self.graphs, self.label = self._load_graph(mat_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Get graph and label by index\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Item index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (dgl.DGLGraph, Tensor)\n",
    "        \"\"\"\n",
    "        return self.graphs[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of graphs in the dataset\"\"\"\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def num_tasks(self):\n",
    "    \"\"\"Number of labels for each graph, i.e. number of prediction tasks.\"\"\"\n",
    "    return 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 노드 분류 데이터셋 프로세싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLBuiltinDataset\n",
    "from dgl.data.utils import _get_dgl_url\n",
    "\n",
    "class CitationGraphDataset(DGLBuiltinDataset):\n",
    "    _urls = {\n",
    "        'cora_v2' : 'dataset/cora_v2.zip',\n",
    "        'citeseer' : 'dataset/citeseer.zip',\n",
    "        'pubmed' : 'dataset/pubmed.zip',\n",
    "    }\n",
    "\n",
    "    def __init__(self, name, raw_dir=None, force_reload=False, verbose=True):\n",
    "        assert name.lower() in ['cora', 'citeseer', 'pubmed']\n",
    "        if name.lower() == 'cora':\n",
    "            name = 'cora_v2'\n",
    "        url = _get_dgl_url(self._urls[name])\n",
    "        super(CitationGraphDataset, self).__init__(name,\n",
    "                                                   url=url,\n",
    "                                                   raw_dir=raw_dir,\n",
    "                                                   force_reload=force_reload,\n",
    "                                                   verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        # Skip some processing code\n",
    "        # === data processing skipped ===\n",
    "\n",
    "        # build graph\n",
    "        g = dgl.graph(graph)\n",
    "        # splitting masks\n",
    "        g.ndata['train_mask'] = train_mask\n",
    "        g.ndata['val_mask'] = val_mask\n",
    "        g.ndata['test_mask'] = test_mask\n",
    "        # node labels\n",
    "        g.ndata['label'] = torch.tensor(labels)\n",
    "        # node features\n",
    "        g.ndata['feat'] = torch.tensor(_preprocess_features(features),\n",
    "                                       dtype=F.data_type_dict['float32'])\n",
    "        self._num_tasks = onehot_labels.shape[1]\n",
    "        self._labels = labels\n",
    "        # reorder graph to obtain better locality.\n",
    "        self._g = dgl.reorder_graph(g)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx == 0, \"This dataset has only one graph\"\n",
    "        return self._g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yang.yoonjeong\\Desktop\\geom-ex\\DGL\\45-gd-pipeline-gnn-training.ipynb 셀 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mload_data\u001b[39;00m \u001b[39mimport\u001b[39;00m CiteseerGraphDataset\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataset \u001b[39m=\u001b[39m CiteseerGraphDataset(raw_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m graph \u001b[39m=\u001b[39m dataset[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\yang.yoonjeong\\Desktop\\geom-ex\\DGL\\load_data.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39msys\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m save_graphs, load_graphs, save_info, load_info, makedirs, _get_dgl_url\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m generate_mask_tensor\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecate_property, deprecate_function\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from load_data import CiteseerGraphDataset\n",
    "dataset = CiteseerGraphDataset(raw_dir='')\n",
    "graph = dataset[0]\n",
    "\n",
    "# get split masks\n",
    "train_mask = graph.ndata['train_mask']\n",
    "val_mask = graph.ndata['val_mask']\n",
    "test_mask = graph.ndata['test_mask']\n",
    "\n",
    "# get node features\n",
    "feats = graph.ndata['feat']\n",
    "\n",
    "# get labels\n",
    "labels = graph.ndata['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 링크 예측 데이터셋 프로세싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for creating Link Prediction datasets\n",
    "class KnowledgeGraphDataset(DGLBuiltinDataset):\n",
    "    def __init__(self, name, reverse=True, raw_dir=None, force_reload=False, verbose=True):\n",
    "        self._name = name\n",
    "        self.reverse = reverse\n",
    "        url = _get_dgl_url('dataset/') + '{}.tgz'.format(name)\n",
    "        super(KnowledgeGraphDataset, self).__init__(name,\n",
    "                                                    url=url,\n",
    "                                                    raw_dir=raw_dir,\n",
    "                                                    force_reload=force_reload,\n",
    "                                                    verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        # Skip some processing code\n",
    "        # === data processing skipped ===\n",
    "\n",
    "        # splitting mask\n",
    "        g.edata['train_mask'] = train_mask\n",
    "        g.edata['val_mask'] = val_mask\n",
    "        g.edata['test_mask'] = test_mask\n",
    "        # edge type\n",
    "        g.edata['etype'] = etype\n",
    "        # node type\n",
    "        g.ndata['ntype'] = ntype\n",
    "        self._g = g\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx == 0, \"This dataset has only one graph\"\n",
    "        return self._g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# entities: 14541\n",
      "# relations: 237\n",
      "# training edges: 272115\n",
      "# validation edges: 17535\n",
      "# testing edges: 20466\n",
      "Done loading data from cached files.\n"
     ]
    },
    {
     "ename": "DGLError",
     "evalue": "Invalid form: tensor([     0,      1,      2,  ..., 620214, 620215, 620216]). Must be \"all\", \"uv\" or \"eid\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yang.yoonjeong\\Desktop\\geom-ex\\DGL\\45-gd-pipeline-gnn-training.ipynb 셀 14\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_mask \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39medata[\u001b[39m'\u001b[39m\u001b[39mtrain_mask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_idx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnonzero(train_mask, as_tuple\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m src, dst \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39;49medges(train_idx)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# get edge types in training set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m rel \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39medata[\u001b[39m'\u001b[39m\u001b[39metype\u001b[39m\u001b[39m'\u001b[39m][train_idx]\n",
      "File \u001b[1;32mc:\\Users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages\\dgl\\view.py:166\u001b[0m, in \u001b[0;36mHeteroEdgeView.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    165\u001b[0m     \u001b[39m\"\"\"Return all the edges.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39mall_edges(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages\\dgl\\heterograph.py:3425\u001b[0m, in \u001b[0;36mDGLHeteroGraph.all_edges\u001b[1;34m(self, form, order, etype)\u001b[0m\n\u001b[0;32m   3423\u001b[0m     \u001b[39mreturn\u001b[39;00m eid\n\u001b[0;32m   3424\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3425\u001b[0m     \u001b[39mraise\u001b[39;00m DGLError(\u001b[39m'\u001b[39m\u001b[39mInvalid form: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. Must be \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39muv\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m or \u001b[39m\u001b[39m\"\u001b[39m\u001b[39meid\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(form))\n",
      "\u001b[1;31mDGLError\u001b[0m: Invalid form: tensor([     0,      1,      2,  ..., 620214, 620215, 620216]). Must be \"all\", \"uv\" or \"eid\"."
     ]
    }
   ],
   "source": [
    "from dgl.data import FB15k237Dataset\n",
    "\n",
    "# load data\n",
    "dataset = FB15k237Dataset()\n",
    "graph = dataset[0]\n",
    "\n",
    "# get training mask\n",
    "train_mask = graph.edata['train_mask']\n",
    "train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "src, dst = graph.edges(train_idx)\n",
    "# get edge types in training set\n",
    "rel = graph.edata['etype'][train_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 데이터 저장과 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dgl import save_graphs, load_graphs\n",
    "from dgl.data.utils import makedirs, save_info, load_info\n",
    "\n",
    "def save(self):\n",
    "    # save graphs and labels\n",
    "    graph_path = os.path.join(self.save_path, self.mode + '_dgl_graph.bin')\n",
    "    save_graphs(graph_path, self.graphs, {'labels': self.labels})\n",
    "    # save other information in python dict\n",
    "    info_path = os.path.join(self.save_path, self.mode + '_info.pkl')\n",
    "    save_info(info_path, {'num_classes': self.num_classes})\n",
    "\n",
    "def load(self):\n",
    "    # load processed data from directory `self.save_path`\n",
    "    graph_path = os.path.join(self.save_path, self.mode + '_dgl_graph.bin')\n",
    "    self.graphs, label_dict = load_graphs(graph_path)\n",
    "    self.labels = label_dict['labels']\n",
    "    info_path = os.path.join(self.save_path, self.mode + '_info.pkl')\n",
    "    self.num_classes = load_info(info_path)['num_classes']\n",
    "\n",
    "def has_cache(self):\n",
    "    # check whether there are processed data in `self.save_path`\n",
    "    graph_path = os.path.join(self.save_path, self.mode + '_dgl_graph.bin')\n",
    "    info_path = os.path.join(self.save_path, self.mode + '_info.pkl')\n",
    "    return os.path.exists(graph_path) and os.path.exists(info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 `ogb` 패키지를 사용해서 OGB 데이터셋들 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ogb in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (1.3.5)\n",
      "Requirement already satisfied: outdated>=0.2.0 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from ogb) (0.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from ogb) (1.26.11)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from ogb) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from ogb) (1.23.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from ogb) (1.12.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from ogb) (1.5.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from ogb) (1.1.3)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from ogb) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from outdated>=0.2.0->ogb) (2.28.1)\n",
      "Requirement already satisfied: setuptools>=44 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from outdated>=0.2.0->ogb) (63.4.1)\n",
      "Requirement already satisfied: littleutils in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from pandas>=0.24.0->ogb) (2022.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.9.3)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.6.0->ogb) (4.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.29.0->ogb) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Graph Property Prediction datasets in OGB\n",
    "import dgl\n",
    "import torch\n",
    "from ogb.graphproppred import DglGraphPropPredDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "\n",
    "def _collate_fn(batch):\n",
    "    # batch is a list of tuple (graph, label)\n",
    "    graphs = [e[0] for e in batch]\n",
    "    g = dgl.batch(graphs)\n",
    "    labels = [e[1] for e in batch]\n",
    "    labels = torch.stack(labels, 0)\n",
    "    return g, labels\n",
    "\n",
    "# load dataset\n",
    "dataset = DglGraphPropPredDataset(name='ogbg-molhiv')\n",
    "split_idx = dataset.get_idx_split()\n",
    "# dataloader\n",
    "train_loader = GraphDataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, collate_fn=_collate_fn)\n",
    "valid_loader = GraphDataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, collate_fn=_collate_fn)\n",
    "test_loader = GraphDataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, collate_fn=_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/proteins.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.21 GB: 100%|██████████| 216/216 [00:15<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset\\proteins.zip\n",
      "Loading necessary files...\n",
      "This might take a while.\n",
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into DGL objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "# Load Node Property Prediction datasets in OGB\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset(name='ogbn-proteins')\n",
    "split_idx = dataset.get_idx_split()\n",
    "\n",
    "# there is only one graph in Node Property Prediction datasets\n",
    "g, labels = dataset[0]\n",
    "# get split labels\n",
    "train_label = dataset.labels[split_idx['train']]\n",
    "valid_label = dataset.labels[split_idx['valid']]\n",
    "test_label = dataset.labels[split_idx['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/linkproppred/ppassoc.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.38 GB: 100%|██████████| 388/388 [01:15<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset\\ppassoc.zip\n",
      "Loading necessary files...\n",
      "This might take a while.\n",
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into DGL objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "dict_keys(['edge'])\n",
      "dict_keys(['edge', 'edge_neg'])\n",
      "dict_keys(['edge', 'edge_neg'])\n"
     ]
    }
   ],
   "source": [
    "# Load Link Property Prediction datasets in OGB\n",
    "from ogb.linkproppred import DglLinkPropPredDataset\n",
    "\n",
    "dataset = DglLinkPropPredDataset(name='ogbl-ppa')\n",
    "split_edge = dataset.get_edge_split()\n",
    "\n",
    "graph = dataset[0]\n",
    "print(split_edge['train'].keys())\n",
    "print(split_edge['valid'].keys())\n",
    "print(split_edge['test'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------\n",
    "# 5장: 그래프 뉴럴 네트워크 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C:\\Users\\yang.yoonjeong\\.dgl\\citeseer.zip from https://data.dgl.ai/dataset/citeseer.zip...\n",
      "Extracting file to C:\\Users\\yang.yoonjeong\\.dgl\\citeseer\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 3327\n",
      "  NumEdges: 9228\n",
      "  NumFeats: 3703\n",
      "  NumClasses: 6\n",
      "  NumTrainingSamples: 120\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yang.yoonjeong\\anaconda3\\envs\\torch\\lib\\site-packages\\dgl\\data\\citation_graph.py:287: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "dataset = dgl.data.CiteseerGraphDataset()\n",
    "graph = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heterogeneous 그래프\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "n_users = 1000\n",
    "n_items = 500\n",
    "n_follows = 3000\n",
    "n_clicks = 5000\n",
    "n_dislikes = 500\n",
    "n_hetero_features = 10\n",
    "n_user_classes = 5\n",
    "n_max_clicks = 10\n",
    "\n",
    "follow_src = np.random.randint(0, n_users, n_follows)\n",
    "follow_dst = np.random.randint(0, n_users, n_follows)\n",
    "click_src = np.random.randint(0, n_users, n_clicks)\n",
    "click_dst = np.random.randint(0, n_items, n_clicks)\n",
    "dislike_src = np.random.randint(0, n_users, n_dislikes)\n",
    "dislike_dst = np.random.randint(0, n_items, n_dislikes)\n",
    "\n",
    "hetero_graph = dgl.heterograph({\n",
    "    ('user', 'follow', 'user'): (follow_src, follow_dst),\n",
    "    ('user', 'followed-by', 'user'): (follow_dst, follow_src),\n",
    "    ('user', 'click', 'item'): (click_src, click_dst),\n",
    "    ('item', 'clicked-by', 'user'): (click_dst, click_src),\n",
    "    ('user', 'dislike', 'item'): (dislike_src, dislike_dst),\n",
    "    ('item', 'disliked-by', 'user'): (dislike_dst, dislike_src)})\n",
    "\n",
    "hetero_graph.nodes['user'].data['feature'] = torch.randn(n_users, n_hetero_features)\n",
    "hetero_graph.nodes['item'].data['feature'] = torch.randn(n_items, n_hetero_features)\n",
    "hetero_graph.nodes['user'].data['label'] = torch.randint(0, n_user_classes, (n_users,))\n",
    "hetero_graph.edges['click'].data['label'] = torch.randint(1, n_max_clicks, (n_clicks,)).float()\n",
    "# randomly generate training masks on user nodes and click edges\n",
    "hetero_graph.nodes['user'].data['train_mask'] = torch.zeros(n_users, dtype=torch.bool).bernoulli(0.6)\n",
    "hetero_graph.edges['click'].data['train_mask'] = torch.zeros(n_clicks, dtype=torch.bool).bernoulli(0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 노드 분류/리그래션(Regression)\n",
    "#### (1) 뉴럴 네트워크 모델 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct a two-layer GNN model\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.SAGEConv(\n",
    "            in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')\n",
    "        self.conv2 = dglnn.SAGEConv(\n",
    "            in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(graph, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 학습 룹(loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = graph.ndata['feat']\n",
    "node_labels = graph.ndata['label']\n",
    "train_mask = graph.ndata['train_mask']\n",
    "valid_mask = graph.ndata['val_mask']\n",
    "test_mask = graph.ndata['test_mask']\n",
    "n_features = node_features.shape[1]\n",
    "n_labels = int(node_labels.max().item() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, graph, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7920030355453491\n",
      "1.777983546257019\n",
      "1.7642617225646973\n",
      "1.750003695487976\n",
      "1.7349681854248047\n",
      "1.7191489934921265\n",
      "1.7025129795074463\n",
      "1.685144305229187\n",
      "1.6671029329299927\n",
      "1.6483896970748901\n"
     ]
    }
   ],
   "source": [
    "model = SAGE(in_feats=n_features, hid_feats=100, out_feats=n_labels)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    # forward propagation by using all nodes\n",
    "    logits = model(graph, node_features)\n",
    "    # compute loss\n",
    "    loss = F.cross_entropy(logits[train_mask], node_labels[train_mask])\n",
    "    # compute validation accuracy\n",
    "    acc = evaluate(model, graph, node_features, node_labels, valid_mask)\n",
    "    # backward propagation\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())\n",
    "\n",
    "    # Save model if necessary.  Omitted in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Heterogeneous 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Heterograph Conv model\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, hid_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(hid_feats, out_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = {k: F.relu(v) for k, v in h.items()}\n",
    "        h = self.conv2(graph, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RGCN(n_hetero_features, 20, n_user_classes, hetero_graph.etypes)\n",
    "user_feats = hetero_graph.nodes['user'].data['feature']\n",
    "item_feats = hetero_graph.nodes['item'].data['feature']\n",
    "labels = hetero_graph.nodes['user'].data['label']\n",
    "train_mask = hetero_graph.nodes['user'].data['train_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = {'user': user_feats, 'item': item_feats}\n",
    "h_dict = model(hetero_graph, {'user': user_feats, 'item': item_feats})\n",
    "h_user = h_dict['user']\n",
    "h_item = h_dict['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.001187324523926\n",
      "1.9803088903427124\n",
      "1.9600907564163208\n",
      "1.940536618232727\n",
      "1.921686053276062\n"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    # forward propagation by using all nodes and extracting the user embeddings\n",
    "    logits = model(hetero_graph, node_features)['user']\n",
    "    # compute loss\n",
    "    loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "    # Compute validation accuracy.  Omitted in this example.\n",
    "    # backward propagation\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())\n",
    "\n",
    "    # Save model if necessary.  Omitted in the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 에지 분류 및 리그레션(Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = np.random.randint(0, 100, 500)\n",
    "dst = np.random.randint(0, 100, 500)\n",
    "# make it symmetric\n",
    "edge_pred_graph = dgl.graph((np.concatenate([src, dst]), np.concatenate([dst, src])))\n",
    "# synthetic node and edge features, as well as edge labels\n",
    "edge_pred_graph.ndata['feature'] = torch.randn(100, 10)\n",
    "edge_pred_graph.edata['feature'] = torch.randn(1000, 10)\n",
    "edge_pred_graph.edata['label'] = torch.randn(1000)\n",
    "# synthetic train-validation-test splits\n",
    "edge_pred_graph.edata['train_mask'] = torch.zeros(1000, dtype=torch.bool).bernoulli(0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 노드 분류 모델과 구현상의 차이점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "class DotProductPredictor(nn.Module):\n",
    "    def forward(self, graph, h):\n",
    "        # h contains the node representations computed from the GNN defined\n",
    "        # in the node classification section (Section 5.1).\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features * 2, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        score = self.W(torch.cat([h_u, h_v], 1))\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        # h contains the node representations computed from the GNN defined\n",
    "        # in the node classification section (Section 5.1).\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 학습 룹(loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.sage = SAGE(in_features, hidden_features, out_features)\n",
    "        self.pred = DotProductPredictor()\n",
    "    def forward(self, g, x):\n",
    "        h = self.sage(g, x)\n",
    "        return self.pred(g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.42076110839844\n",
      "118.69367980957031\n",
      "112.36096954345703\n",
      "106.41353607177734\n",
      "100.83235931396484\n",
      "95.5983657836914\n",
      "90.6906509399414\n",
      "86.10187530517578\n",
      "81.80551147460938\n",
      "77.78719329833984\n"
     ]
    }
   ],
   "source": [
    "node_features = edge_pred_graph.ndata['feature']\n",
    "edge_label = edge_pred_graph.edata['label']\n",
    "train_mask = edge_pred_graph.edata['train_mask']\n",
    "model = Model(10, 20, 5)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(10):\n",
    "    pred = model(edge_pred_graph, node_features)\n",
    "    loss = ((pred[train_mask] - edge_label[train_mask]) ** 2).mean()\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Heterogeneous 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroDotProductPredictor(nn.Module):\n",
    "    def forward(self, graph, h, etype):\n",
    "        # h contains the node representations for each edge type computed from\n",
    "        # the GNN for heterogeneous graphs defined in the node classification\n",
    "        # section (Section 5.1).\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h   # assigns 'h' of all node types in one shot\n",
    "            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'), etype=etype)\n",
    "            return graph.edges[etype].data['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroMLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features * 2, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        score = self.W(torch.cat([h_u, h_v], 1))\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, graph, h, etype):\n",
    "        # h contains the node representations for each edge type computed from\n",
    "        # the GNN for heterogeneous graphs defined in the node classification\n",
    "        # section (Section 5.1).\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h   # assigns 'h' of all node types in one shot\n",
    "            graph.apply_edges(self.apply_edges, etype=etype)\n",
    "            return graph.edges[etype].data['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, rel_names):\n",
    "        super().__init__()\n",
    "        self.sage = RGCN(in_features, hidden_features, out_features, rel_names)\n",
    "        self.pred = HeteroDotProductPredictor()\n",
    "    def forward(self, g, x, etype):\n",
    "        h = self.sage(g, x)\n",
    "        return self.pred(g, h, etype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(10, 20, 5, hetero_graph.etypes)\n",
    "user_feats = hetero_graph.nodes['user'].data['feature']\n",
    "item_feats = hetero_graph.nodes['item'].data['feature']\n",
    "label = hetero_graph.edges['click'].data['label']\n",
    "train_mask = hetero_graph.edges['click'].data['train_mask']\n",
    "node_features = {'user': user_feats, 'item': item_feats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.814918518066406\n",
      "31.214706420898438\n",
      "29.682479858398438\n",
      "28.217519760131836\n",
      "26.81751251220703\n",
      "25.47925567626953\n",
      "24.200838088989258\n",
      "22.98187255859375\n",
      "21.825027465820312\n",
      "20.73345184326172\n"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(10):\n",
    "    pred = model(hetero_graph, node_features, 'click')\n",
    "    loss = ((pred[train_mask] - label[train_mask]) ** 2).mean()\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Heterogeneous 그래프의 에지들에 대한 에지 타입 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_graph = hetero_graph['user', :, 'item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_label = dec_graph.edata[dgl.ETYPE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroMLPPredictor(nn.Module):\n",
    "    def __init__(self, in_dims, n_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_dims * 2, n_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        x = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
    "        y = self.W(x)\n",
    "        return {'score': y}\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        # h contains the node representations for each edge type computed from\n",
    "        # the GNN for heterogeneous graphs defined in the node classification\n",
    "        # section (Section 5.1).\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h   # assigns 'h' of all node types in one shot\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, rel_names):\n",
    "        super().__init__()\n",
    "        self.sage = RGCN(in_features, hidden_features, out_features, rel_names)\n",
    "        self.pred = HeteroMLPPredictor(out_features, len(rel_names))\n",
    "    def forward(self, g, x, dec_graph):\n",
    "        h = self.sage(g, x)\n",
    "        return self.pred(dec_graph, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0574491024017334\n",
      "2.010658025741577\n",
      "1.9644696712493896\n",
      "1.9188644886016846\n",
      "1.8738352060317993\n",
      "1.8294036388397217\n",
      "1.785575270652771\n",
      "1.7423341274261475\n",
      "1.6996670961380005\n",
      "1.6575711965560913\n"
     ]
    }
   ],
   "source": [
    "model = Model(10, 20, 5, hetero_graph.etypes)\n",
    "user_feats = hetero_graph.nodes['user'].data['feature']\n",
    "item_feats = hetero_graph.nodes['item'].data['feature']\n",
    "node_features = {'user': user_feats, 'item': item_feats}\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(10):\n",
    "    logits = model(hetero_graph, node_features, dec_graph)\n",
    "    loss = F.cross_entropy(logits, edge_label)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 링크 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductPredictor(nn.Module):\n",
    "    def forward(self, graph, h):\n",
    "        # h contains the node representations computed from the GNN defined\n",
    "        # in the node classification section (Section 5.1).\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 학습 룹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_negative_graph(graph, k):\n",
    "    src, dst = graph.edges()\n",
    "\n",
    "    neg_src = src.repeat_interleave(k)\n",
    "    neg_dst = torch.randint(0, graph.num_nodes(), (len(src) * k,))\n",
    "    return dgl.graph((neg_src, neg_dst), num_nodes=graph.num_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.sage = SAGE(in_features, hidden_features, out_features)\n",
    "        self.pred = DotProductPredictor()\n",
    "    def forward(self, g, neg_g, x):\n",
    "        h = self.sage(g, x)\n",
    "        return self.pred(g, h), self.pred(neg_g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9989525675773621\n",
      "0.9975232481956482\n",
      "0.995322048664093\n",
      "0.9922455549240112\n",
      "0.9880377054214478\n",
      "0.9825226068496704\n",
      "0.975499153137207\n",
      "0.9666041731834412\n",
      "0.955389142036438\n",
      "0.9416782855987549\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(pos_score, neg_score):\n",
    "    # Margin loss\n",
    "    n_edges = pos_score.shape[0]\n",
    "    return (1 - pos_score + neg_score.view(n_edges, -1)).clamp(min=0).mean()\n",
    "\n",
    "node_features = graph.ndata['feat']\n",
    "n_features = node_features.shape[1]\n",
    "k = 5\n",
    "model = Model(n_features, 100, 100)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(10):\n",
    "    negative_graph = construct_negative_graph(graph, k)\n",
    "    pos_score, neg_score = model(graph, negative_graph, node_features)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings = model.sage(graph, node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Heterogeneous 그래프들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroDotProductPredictor(nn.Module):\n",
    "    def forward(self, graph, h, etype):\n",
    "        # h contains the node representations for each node type computed from\n",
    "        # the GNN defined in the previous section (Section 5.1).\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'), etype=etype)\n",
    "            return graph.edges[etype].data['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_negative_graph(graph, k, etype):\n",
    "    utype, _, vtype = etype\n",
    "    src, dst = graph.edges(etype=etype)\n",
    "    neg_src = src.repeat_interleave(k)\n",
    "    neg_dst = torch.randint(0, graph.num_nodes(vtype), (len(src) * k,))\n",
    "    return dgl.heterograph(\n",
    "        {etype: (neg_src, neg_dst)},\n",
    "        num_nodes_dict={ntype: graph.num_nodes(ntype) for ntype in graph.ntypes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, rel_names):\n",
    "        super().__init__()\n",
    "        self.sage = RGCN(in_features, hidden_features, out_features, rel_names)\n",
    "        self.pred = HeteroDotProductPredictor()\n",
    "    def forward(self, g, neg_g, x, etype):\n",
    "        h = self.sage(g, x)\n",
    "        return self.pred(g, h, etype), self.pred(neg_g, h, etype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2252888679504395\n",
      "1.2004209756851196\n",
      "1.1815917491912842\n",
      "1.17046058177948\n",
      "1.1512457132339478\n",
      "1.1469422578811646\n",
      "1.134092092514038\n",
      "1.1217029094696045\n",
      "1.1240384578704834\n",
      "1.1099580526351929\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(pos_score, neg_score):\n",
    "    # Margin loss\n",
    "    n_edges = pos_score.shape[0]\n",
    "    return (1 - pos_score + neg_score.view(n_edges, -1)).clamp(min=0).mean()\n",
    "\n",
    "k = 5\n",
    "model = Model(10, 20, 5, hetero_graph.etypes)\n",
    "user_feats = hetero_graph.nodes['user'].data['feature']\n",
    "item_feats = hetero_graph.nodes['item'].data['feature']\n",
    "node_features = {'user': user_feats, 'item': item_feats}\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(10):\n",
    "    negative_graph = construct_negative_graph(hetero_graph, k, ('user', 'click', 'item'))\n",
    "    pos_score, neg_score = model(hetero_graph, negative_graph, node_features, ('user', 'click', 'item'))\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 그래프 분류\n",
    "- 그래프 분류 파이프라인\n",
    "\n",
    "<img src = 'https://data.dgl.ai/tutorial/batch/graph_classifier.png'>\n",
    "\n",
    "- 일반적인 방법은 (왼쪽부터 오른쪽으로 진행):\n",
    "    - 그래프들의 배치를 준비한다\n",
    "    - 그래프들의 배치에 메시지 전달을 수행해서 노드/에지 피쳐를 업데이트한다\n",
    "    - 노드/에지 피쳐들을 모두 합쳐서 그래프 수준의 representation들을 만든다\n",
    "    - 그래프 수준의 representation들을 사용해서 그래프들을 분류한다\n",
    "\n",
    "##### 그래프들의 배치(batch)\n",
    "- 배치 그래프\\\n",
    "<img src = \"https://data.dgl.ai/tutorial/batch/batch.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 4, 4, 4, 5]), tensor([1, 2, 3, 4, 5, 6, 4]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "\n",
    "g1 = dgl.graph((th.tensor([0, 1, 2]), th.tensor([1, 2, 3])))\n",
    "g2 = dgl.graph((th.tensor([0, 0, 0, 1]), th.tensor([0, 1, 2, 0])))\n",
    "\n",
    "bg = dgl.batch([g1, g2])\n",
    "bg\n",
    "# Graph(num_nodes=7, num_edges=7,\n",
    "#       ndata_schemes={}\n",
    "#       edata_schemes={})\n",
    "bg.batch_size\n",
    "# 2\n",
    "bg.batch_num_nodes()\n",
    "# tensor([4, 3])\n",
    "bg.batch_num_edges()\n",
    "# tensor([3, 4])\n",
    "bg.edges()\n",
    "# (tensor([0, 1, 2, 4, 4, 4, 5], tensor([1, 2, 3, 4, 5, 6, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 그래프 리드아웃(readout)\n",
    "\n",
    "#### (1) 뉴럴 네트워크 모델 작성하기\n",
    "##### 배치 그래프에 연산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "g1 = dgl.graph(([0, 1], [1, 0]))\n",
    "g1.ndata['h'] = torch.tensor([1., 2.])\n",
    "g2 = dgl.graph(([0, 1], [1, 2]))\n",
    "g2.ndata['h'] = torch.tensor([1., 2., 3.])\n",
    "\n",
    "dgl.readout_nodes(g1, 'h')\n",
    "# tensor([3.])  # 1 + 2\n",
    "\n",
    "bg = dgl.batch([g1, g2])\n",
    "dgl.readout_nodes(bg, 'h')\n",
    "# tensor([3., 6.])  # [1 + 2, 1 + 2 + 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 1., 2., 3.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg.ndata['h']\n",
    "# tensor([1., 2., 1., 2., 3.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn.pytorch as dglnn\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = dglnn.GraphConv(in_dim, hidden_dim)\n",
    "        self.conv2 = dglnn.GraphConv(hidden_dim, hidden_dim)\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # Apply graph convolution and activation.\n",
    "        h = F.relu(self.conv1(g, h))\n",
    "        h = F.relu(self.conv2(g, h))\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Calculate graph representation by average readout.\n",
    "            hg = dgl.mean_nodes(g, 'h')\n",
    "            return self.classify(hg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 학습 룹\n",
    "##### 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C:\\Users\\yang.yoonjeong\\.dgl\\GINDataset.zip from https://raw.githubusercontent.com/weihua916/powerful-gnns/master/dataset.zip...\n",
      "Extracting file to C:\\Users\\yang.yoonjeong\\.dgl\\GINDataset\n"
     ]
    }
   ],
   "source": [
    "import dgl.data\n",
    "dataset = dgl.data.GINDataset('MUTAG', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.dataloading import GraphDataLoader\n",
    "dataloader = GraphDataLoader(\n",
    "    dataset,\n",
    "    batch_size=1024,\n",
    "    drop_last=False,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Only an example, 7 is the input feature size\n",
    "model = Classifier(7, 20, 5)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(20):\n",
    "    for batched_graph, labels in dataloader:\n",
    "        feats = batched_graph.ndata['attr']\n",
    "        logits = model(batched_graph, feats)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Heterogeneous 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, hid_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(hid_feats, out_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs is features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = {k: F.relu(v) for k, v in h.items()}\n",
    "        h = self.conv2(graph, h)\n",
    "        return h\n",
    "\n",
    "class HeteroClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, rel_names):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rgcn = RGCN(in_dim, hidden_dim, hidden_dim, rel_names)\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['feat']\n",
    "        h = self.rgcn(g, h)\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Calculate graph representation by average readout.\n",
    "            hg = 0\n",
    "            for ntype in g.ntypes:\n",
    "                hg = hg + dgl.mean_nodes(g, 'h', ntype=ntype)\n",
    "            return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'etypes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yang.yoonjeong\\Desktop\\geom-ex\\DGL\\45-gd-pipeline-gnn-training.ipynb 셀 81\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#Y143sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# etypes is the list of edge types as strings.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#Y143sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m HeteroClassifier(\u001b[39m10\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m5\u001b[39m, etypes)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#Y143sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m opt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yang.yoonjeong/Desktop/geom-ex/DGL/45-gd-pipeline-gnn-training.ipynb#Y143sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'etypes' is not defined"
     ]
    }
   ],
   "source": [
    "# etypes is the list of edge types as strings.\n",
    "model = HeteroClassifier(10, 20, 5, etypes)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(20):\n",
    "    for batched_graph, labels in dataloader:\n",
    "        logits = model(batched_graph)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fec2c05703d0080e6fd294faeab87a4e3eaf52cde4adf51bddbbb91aae929392"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
